{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyse des erreurs de prédictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Ce notebook contient un bout de code permettant d'afficher les mauvaises prédictions de nos modèles de classifications. Il affiche les différents panneaux de signalisation mal prédits par le modèle, avec la prédiction du modèle et la vraie classe du panneau."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Etape 1\n",
    "Changer le répertoire de travail en utilisant le code suivant:\n",
    "```python\n",
    "os.chdir(f'/home/axel/Bureau/5_Mod_IA/Semestre_1_INSA/Statistiques_Grande_Dimension_Apprentissage_Profond/Projet/Projet_HDDL_2/Commun/Commun_Axel')\n",
    "```\n",
    "### Etape 2\n",
    "Exécute le code cellule par cellule. Le code est censé importer les modules nécéssaires, importer les données du type voulu (Dangers, Obligations, Interdictions, Indications....), importer le modèle associé, et enfin prédire les panneaux de signalisation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importation des modules et définition de l'emplacement du workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import sys\n",
    "import os\n",
    "\n",
    "os.chdir(f'/home/axel/Bureau/5_Mod_IA/Semestre_1_INSA/Statistiques_Grande_Dimension_Apprentissage_Profond/Projet/Projet_HDDL_2/Commun/Commun_Axel')\n",
    "print(os.getcwd())\n",
    "def check_path_exists(Path,Message):\n",
    "    if not os.path.exists(Path):\n",
    "        print(Message)\n",
    "        sys.exit(1)\n",
    "\n",
    "# Cette partie permet d'ajouter le dossier parent au path, pour pouvoir importer les modules que nous avons créés. ça peut être soit un chemin\n",
    "# absolu, soit un chemin relatif. Ici, c'est un chemin absolu.\n",
    "Path_Modules = \"./Modules\"\n",
    "Path_Models = \"./Models\"\n",
    "check_path_exists(Path_Modules,\"Le chemin spécifié pour importer les modules n'existe pas. Il faut surement le modifier.\")\n",
    "\n",
    "sys.path.append(Path_Modules)\n",
    "sys.path.append(Path_Models)\n",
    "# Importer les module\n",
    "from Preprocessing import *\n",
    "from Resnet import *\n",
    "from Train import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Définitions de fonctions utiles pour la suite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_encoding_dictionaries(path,title_label_to_int ,title_int_to_label):\n",
    "    \n",
    "    def check_path_exists(path,message):\n",
    "        if not os.path.exists(path):\n",
    "            raise ValueError(message)\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    check_path_exists(path,\"The specified path for the encoding dictionaries does not exist. You probably need to modify it.\")\n",
    "\n",
    "    # Load JSON into dictionaries\n",
    "    with open(path+title_label_to_int+\".json\", \"r\") as read_file:\n",
    "        label_to_int = json.load(read_file)\n",
    "\n",
    "    with open(path+title_int_to_label+\".json\", \"r\") as read_file:\n",
    "        int_to_label = json.load(read_file)\n",
    "\n",
    "    return label_to_int,int_to_label\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def model_labels(Type = \"Indications\"):\n",
    "    Image_Information_Path = \"./Anotations/\"\n",
    "    Name_Image_Information = \"Image_Information.csv\"\n",
    "    Name_Image_Information_Train = \"Image_Information_Train.csv\"\n",
    "    Name_Image_Information_Test = \"Image_Information_Test.csv\"\n",
    "\n",
    "    # Checker si le fichier existe\n",
    "    #check_path_exists(Image_Information_Path,\"Le chemin spécifié pour le DataFrame n'existe pas. Il faut surement le modifier.\")\n",
    "\n",
    "    # Charger le DataFrame\n",
    "    Image_Information_Dataframe = pd.read_csv(Image_Information_Path+Name_Image_Information)\n",
    "    Image_Information_Dataframe_Train = pd.read_csv(Image_Information_Path+Name_Image_Information_Train)\n",
    "    Image_Information_Dataframe_Test = pd.read_csv(Image_Information_Path+Name_Image_Information_Test)\n",
    "\n",
    "\n",
    "    if Type == \"Types\": \n",
    "        pass\n",
    "    else: \n",
    "        Image_Information_Dataframe_Test = Image_Information_Dataframe_Test[Image_Information_Dataframe_Test[\"Type\"]== Type]\n",
    "        Image_Information_Dataframe_Train = Image_Information_Dataframe_Train[Image_Information_Dataframe_Train[\"Type\"]== Type]\n",
    "        Image_Information_Dataframe = Image_Information_Dataframe[Image_Information_Dataframe[\"Type\"]== Type]\n",
    "\n",
    "    print(f\"Nombre d'images dans le dataset: {Image_Information_Dataframe.shape[0]}\")\n",
    "    print(f\"Nombre d'images dans le dataset de train: {Image_Information_Dataframe_Train.shape[0]}\")\n",
    "    print(f\"Nombre d'images dans le dataset de test: {Image_Information_Dataframe_Test.shape[0]}\")\n",
    "\n",
    "\n",
    "    # Preprocessing des images\n",
    "    resize=True\n",
    "    size=(224,224)\n",
    "    normalize=True\n",
    "    # Nb_Images_To_Import = 5\n",
    "    Frac_Images_To_Import = 1\n",
    "\n",
    "    images_train, types_train, sublabels_train, labels_train = import_images_tensor(Image_Information_Dataframe_Train.sample(frac=Frac_Images_To_Import),\"Relative_Path\", \"Type\", \"Sublabel\", \"Label\",resize=resize, size=size,normalize=normalize)\n",
    "    images_test , types_test, sublabels_test, labels_test = import_images_tensor(Image_Information_Dataframe_Test.sample(frac=Frac_Images_To_Import),\"Relative_Path\", \"Type\", \"Sublabel\", \"Label\",resize=resize, size=size,normalize=normalize)\n",
    "\n",
    "\n",
    "\n",
    "    # # # Création des dictionnaires d'encodage\n",
    "    save_dicts = False\n",
    "    Path_Save_Dicts = f\"./Encoding_Dictionaries/\"\n",
    "\n",
    "    # Load Encoding Dictionaries\n",
    "    types_to_int , int_to_types = load_encoding_dictionaries(Path_Save_Dicts+\"Types/\",\"types_to_int\",\"int_to_types\")\n",
    "\n",
    "    if Type !=\"Types\" :\n",
    "        labels_to_int , int_to_labels = load_encoding_dictionaries(Path_Save_Dicts + Type + \"/\",\"labels_to_int\",\"int_to_labels\")\n",
    "        sublabels_to_int , int_to_sublabels = load_encoding_dictionaries(Path_Save_Dicts + Type + \"/\",\"sublabels_to_int\",\"int_to_sublabels\")\n",
    "    else:\n",
    "        sublabels_to_int , int_to_sublabels = Create_Encoding_Label_Dictionary(sublabels_train,save=save_dicts, path=Path_Save_Dicts,title_label_to_int='sublabels_to_int',title_int_to_label='int_to_sublabels')\n",
    "        labels_to_int , int_to_labels = Create_Encoding_Label_Dictionary(labels_train,save=save_dicts, path=Path_Save_Dicts,title_label_to_int='labels_to_int',title_int_to_label='int_to_labels')\n",
    "    \n",
    "    # # Encodage des labels\n",
    "\n",
    "    types_encoded_train,types_encoded_test = encode_labels(types_train, types_to_int) , encode_labels(types_test, types_to_int)\n",
    "    sublabels_encoded_train,sublabels_encoded_test = encode_labels(sublabels_train, sublabels_to_int) , encode_labels(sublabels_test, sublabels_to_int)\n",
    "    labels_encoded_train,labels_encoded_test = encode_labels(labels_train, labels_to_int) , encode_labels(labels_test, labels_to_int)\n",
    "\n",
    "\n",
    "    # # Creating Dataloaders : Train and Test\n",
    "    dataloader_train, _ = create_dataloader(images_train, types_encoded_train, sublabels_encoded_train, labels_encoded_train, batch_size=32, test_size=0, shuffle=True)\n",
    "    dataloader_test ,_= create_dataloader(images_test, types_encoded_test, sublabels_encoded_test, labels_encoded_test, batch_size=32, test_size=0, shuffle=True)\n",
    "\n",
    "    # # load the model\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Evalutation sur {device}\")\n",
    "    model = torch.load(f'./Saved_Models/Model_{Type}.pth').to(device)\n",
    "    model.eval()\n",
    "\n",
    "\n",
    "    if Type ==\"Types\" : \n",
    "        return model, None, None, int_to_types, dataloader_test, dataloader_train\n",
    "    else:\n",
    "        return model, int_to_labels, int_to_sublabels, int_to_types, dataloader_test, dataloader_train\n",
    "    \n",
    "\n",
    "# REprende tous les mauvais predictions\n",
    "def get_all_false_predictions(model,dataloader,Type):\n",
    "    with torch.no_grad():\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        model.eval()\n",
    "        all_false_predictions = []\n",
    "        for i, data in enumerate(dataloader):\n",
    "            images, types, sublabels, labels = data\n",
    "            images, types, sublabels, labels = images.to(device), types.to(device), sublabels.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            for i in range(len(predicted)):\n",
    "                if Type==\"Types\":\n",
    "                    if predicted[i] != types[i]:\n",
    "                        all_false_predictions.append((images[i],predicted[i],types[i]))\n",
    "        \n",
    "                    # if len(all_false_predictions) == 4:\n",
    "                    #     return all_false_predictions\n",
    "                else:\n",
    "                    if predicted[i] != labels[i]:\n",
    "                        all_false_predictions.append((images[i],predicted[i],labels[i]))\n",
    "        \n",
    "                    # if len(all_false_predictions) == 4:\n",
    "                    #     return all_false_predictions\n",
    "\n",
    "    return all_false_predictions\n",
    "\n",
    "\n",
    "\n",
    "# Afficher les mauvaises predictions\n",
    "\n",
    "\n",
    "def show_false_prediction(int_to_labels,all_false_predictions):\n",
    "\n",
    "    def imshow(img):\n",
    "        npimg = img.cpu().numpy()\n",
    "        plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "\n",
    "    # plt.subplots(len(all_false_predictions), 1, figsize=(12, 12))\n",
    "    for i in range(len(all_false_predictions)):\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        imshow(all_false_predictions[i][0])\n",
    "        plt.title(f\"$\\\\bf{{Predicted}}$: {int_to_labels[str(all_false_predictions[i][1].item())]},     \"\n",
    "                f\"$\\\\bf{{True}}$: {int_to_labels[str(all_false_predictions[i][2].item())]}\", fontsize=14)\n",
    "        plt.axis('off')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse de l'erreur du modèle de prédiction du type de panneau( Dangers, Obligations, Interdictions, Indications....)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_Type, int_to_labels, int_to_sublabels, int_to_types, dataloader_test, dataloader_train = model_labels(\"Types\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "all_false_predictions_types = get_all_false_predictions(model_Type,dataloader_test,'Types')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_false_prediction(int_to_labels=int_to_types,all_false_predictions=all_false_predictions_types)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse de l'erreur Dangers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_Dangers, int_to_labels_Dangers, int_to_sublabels_Dangers, int_to_types, dataloader_test, dataloader_train = model_labels(\"Dangers\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_false_predictions_dangers = get_all_false_predictions(model_Dangers,dataloader_test,'Dangers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_false_prediction(int_to_labels=int_to_sublabels_Dangers,all_false_predictions=all_false_predictions_dangers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse de l'erreur Obligations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_Obligations, int_to_labels_Obligations, int_to_sublabels_Obligations, int_to_types, dataloader_test, dataloader_train = model_labels(\"Obligations\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_false_predictions_obligations = get_all_false_predictions(model_Obligations,dataloader_test,'Obligations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_false_prediction(int_to_labels=int_to_sublabels_Obligations,all_false_predictions=all_false_predictions_obligations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse de l'erreur Interdictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_Interdictions, int_to_labels_Interdictions, int_to_sublabels_Interdictions, int_to_types, dataloader_test, dataloader_train = model_labels(\"Interdictions\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_false_predictions_obligations = get_all_false_predictions(model_Interdictions,dataloader_test,'Interdictions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_false_prediction(int_to_labels=int_to_sublabels_Interdictions,all_false_predictions=all_false_predictions_obligations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse de l'erreur Indications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_Indications, int_to_labels_Indications, int_to_sublabels_Indications, int_to_types, dataloader_test, dataloader_train = model_labels(\"Indications\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_false_predictions_obligations = get_all_false_predictions(model_Indications,dataloader_test,'Indications')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_false_prediction(int_to_labels=int_to_sublabels_Indications,all_false_predictions=all_false_predictions_obligations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse de l'erreur Fin_Interdictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_Fin_Interdictions, int_to_labels_Fin_Interdictions, int_to_sublabels_Fin_Interdictions, int_to_types, dataloader_test, dataloader_train = model_labels(\"Fin_Interdictions\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_false_predictions_obligations = get_all_false_predictions(model_Fin_Interdictions,dataloader_test,'Fin_Interdictions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_false_prediction(int_to_labels=int_to_sublabels_Fin_Interdictions,all_false_predictions=all_false_predictions_obligations)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Personal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
